{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249dc11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from collections import deque\n",
    "from environment import MECEnvironment  \n",
    "from ddqn_agent import DDQNAgent  \n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Initialize environment\n",
    "env = MECEnvironment()  \n",
    "state_size = env.state_size\n",
    "action_size = env.action_size\n",
    "\n",
    "\n",
    "agent = DDQNAgent(state_size, action_size)\n",
    "\n",
    "# Training parameters\n",
    "EPISODES = 5000  \n",
    "BATCH_SIZE = 64  \n",
    "SAVE_PATH = \"ddqn_model.h5\"  \n",
    "TARGET_UPDATE_FREQ = 10  \n",
    "\n",
    "\n",
    "memory = deque(maxlen=20000)\n",
    "\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    state = env.reset()  \n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(env.max_steps):\n",
    "        action = agent.act(state)  \n",
    "        next_state, reward, done, _ = env.step(action)  \n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "        \n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        \n",
    "        if len(memory) > BATCH_SIZE:\n",
    "            minibatch = random.sample(memory, BATCH_SIZE)\n",
    "            agent.replay(minibatch)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    \n",
    "    if episode % TARGET_UPDATE_FREQ == 0:\n",
    "        agent.update_target_network()\n",
    "\n",
    "    print(f\"Episode {episode+1}/{EPISODES} - Reward: {total_reward:.2f}, Epsilon: {agent.epsilon:.4f}\")\n",
    "\n",
    "# Save trained model\n",
    "agent.q_network.save(SAVE_PATH)\n",
    "print(f\"DDQN model weights saved to {SAVE_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
